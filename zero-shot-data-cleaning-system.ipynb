{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/konavagadheeswari/zero-shot-data-cleaning-system?scriptVersionId=281814277\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"ad908a82","metadata":{"papermill":{"duration":0.007998,"end_time":"2025-11-26T02:42:09.173617","exception":false,"start_time":"2025-11-26T02:42:09.165619","status":"completed"},"tags":[]},"source":["<h1 style=\"font-size: 24px;\">Why Develop the Zero-Shot Data Cleaning System?</h1>"]},{"cell_type":"markdown","id":"65baa639","metadata":{"papermill":{"duration":0.006743,"end_time":"2025-11-26T02:42:09.18748","exception":false,"start_time":"2025-11-26T02:42:09.180737","status":"completed"},"tags":[]},"source":["<h2 style=\"font-size: 20px; font-family: 'sans-serif';\"> The primary reason for developing this multi-agent system is to eliminate the single largest bottleneck in data science and analytics: manual, brittle, and non-reusable data cleaning.</h2>\n","\n","*ðŸ›‘ **Pain Point:** Manual data cleaning is slow, error-prone, and non-reusable, bottlenecking data science with custom scripts and creating technical debt.*\n","\n","*âœ… **The Goal:** The system aims for true zero-shot automation by having the AI Planner instantly write and adapt the cleaning script for any unseen dataset.* \n","\n","*ðŸ’° **Core Benefit:** The multi-agent design ensures consistency, speed, and a drastic cost reduction, freeing data scientists to focus on modeling instead of remediation.*"]},{"cell_type":"markdown","id":"0778a395","metadata":{"papermill":{"duration":0.006662,"end_time":"2025-11-26T02:42:09.200982","exception":false,"start_time":"2025-11-26T02:42:09.19432","status":"completed"},"tags":[]},"source":["<h1 style=\"font-size: 24px;\">How to Build the System</h1>"]},{"cell_type":"markdown","id":"4dfa1d2d","metadata":{"papermill":{"duration":0.006771,"end_time":"2025-11-26T02:42:09.215044","exception":false,"start_time":"2025-11-26T02:42:09.208273","status":"completed"},"tags":[]},"source":["<h2 style=\"font-size: 20px; font-family: 'sans-serif';\">The Zero-Shot Data Cleaning architecture separates the LLM's cleaning decisions (creative) from the code's execution (reliable) into three controllable phases</h2>\n","\n","***Define the Contract (Schema):** Establish a strict JSON schema (e.g., Pydantic) that dictates the only valid cleaning commands the LLM can suggest, ensuring predictable communication.*\n","\n","***Build the Executor (Tools):** Write deterministic, tested code functions to reliably execute every command defined in the contract, creating the non-negotiable core transformation engine.*\n","\n","***Integrate and Orchestrate:** Use the LLM to generate the plan (a sequence of commands) and use the Executor to run the steps sequentially against the data, connecting intelligence to action.*"]},{"cell_type":"markdown","id":"401991b4","metadata":{"papermill":{"duration":0.006621,"end_time":"2025-11-26T02:42:09.228483","exception":false,"start_time":"2025-11-26T02:42:09.221862","status":"completed"},"tags":[]},"source":["<h1 style=\"font-size: 24px;\">Imports Libraries & Setup</h1>"]},{"cell_type":"code","execution_count":1,"id":"3cf6e05c","metadata":{"execution":{"iopub.execute_input":"2025-11-26T02:42:09.243982Z","iopub.status.busy":"2025-11-26T02:42:09.243576Z","iopub.status.idle":"2025-11-26T02:42:12.096055Z","shell.execute_reply":"2025-11-26T02:42:12.095083Z"},"papermill":{"duration":2.862305,"end_time":"2025-11-26T02:42:12.097899","exception":false,"start_time":"2025-11-26T02:42:09.235594","status":"completed"},"tags":[]},"outputs":[],"source":["from pydantic import BaseModel, Field\n","from typing import Optional, List, Dict, Any, Tuple, Callable,Literal\n","import pandas as pd\n","import numpy as np\n","import json\n","import os\n","import sys\n","from glob import glob"]},{"cell_type":"markdown","id":"f4498690","metadata":{"papermill":{"duration":0.00682,"end_time":"2025-11-26T02:42:12.111827","exception":false,"start_time":"2025-11-26T02:42:12.105007","status":"completed"},"tags":[]},"source":["<h1 style=\"font-size: 24px;\">External Data Loader Dependency Check</h1>"]},{"cell_type":"markdown","id":"005889f5","metadata":{"papermill":{"duration":0.006668,"end_time":"2025-11-26T02:42:12.125311","exception":false,"start_time":"2025-11-26T02:42:12.118643","status":"completed"},"tags":[]},"source":["*This code exists to check for the external dependency (kagglehub) required to load data from Kaggle.*"]},{"cell_type":"code","execution_count":2,"id":"f69c95f3","metadata":{"execution":{"iopub.execute_input":"2025-11-26T02:42:12.14093Z","iopub.status.busy":"2025-11-26T02:42:12.140476Z","iopub.status.idle":"2025-11-26T02:42:12.580509Z","shell.execute_reply":"2025-11-26T02:42:12.579524Z"},"papermill":{"duration":0.449438,"end_time":"2025-11-26T02:42:12.582048","exception":false,"start_time":"2025-11-26T02:42:12.13261","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["SUCCESS: 'kagglehub' imported successfully.\n"]}],"source":["try:\n","    from kagglehub import dataset_download\n","    print(\"SUCCESS: 'kagglehub' imported successfully.\")\n","    KAGGLEHUB_AVAILABLE = True\n","except ImportError:\n","    print(\"WARNING: 'kagglehub' not found. Data loading from Kaggle will fail.\")\n","    # Define a dummy function to prevent runtime errors if not installed\n","    def dataset_download(id):\n","        raise ImportError(\"kagglehub not installed.\")\n","    KAGGLEHUB_AVAILABLE = False"]},{"cell_type":"markdown","id":"9998a324","metadata":{"papermill":{"duration":0.006878,"end_time":"2025-11-26T02:42:12.596122","exception":false,"start_time":"2025-11-26T02:42:12.589244","status":"completed"},"tags":[]},"source":["<h1 style=\"font-size: 24px;\">Global Data Constants & Setup</h1>"]},{"cell_type":"markdown","id":"756d39dc","metadata":{"papermill":{"duration":0.006765,"end_time":"2025-11-26T02:42:12.609924","exception":false,"start_time":"2025-11-26T02:42:12.603159","status":"completed"},"tags":[]},"source":["*These constants define the source Kaggle dataset, the specific CSV file to extract from it, and the local file path where the final cleaned data will be saved*"]},{"cell_type":"code","execution_count":3,"id":"d4683981","metadata":{"execution":{"iopub.execute_input":"2025-11-26T02:42:12.628245Z","iopub.status.busy":"2025-11-26T02:42:12.627486Z","iopub.status.idle":"2025-11-26T02:42:12.632993Z","shell.execute_reply":"2025-11-26T02:42:12.631877Z"},"papermill":{"duration":0.017645,"end_time":"2025-11-26T02:42:12.634625","exception":false,"start_time":"2025-11-26T02:42:12.61698","status":"completed"},"tags":[]},"outputs":[],"source":["# --- Global Constants & Setup ---\n","KAGGLE_DATASET_ID = \"praveensoni06/1500-latest-movies-datasets-2025\"\n","CSV_FILE_NAME = \"Latest 2025 movies Datasets.csv\" # The file name expected inside the archive\n","OUTPUT_FILE_PATH = \"cleaned_movies_data.csv\""]},{"cell_type":"markdown","id":"be438097","metadata":{"papermill":{"duration":0.007216,"end_time":"2025-11-26T02:42:12.650356","exception":false,"start_time":"2025-11-26T02:42:12.64314","status":"completed"},"tags":[]},"source":["***--- IMPORTANT NOTE FOR DATASET CHANGES ---***\n","\n","*If you wish to use a different Kaggle dataset:*\n","1. Update KAGGLE_DATASET_ID with the 'user_name/dataset_name' found on Kaggle.\n","2.  Update CSV_FILE_NAME with the exact .csv file name that is present INSIDE the downloaded Kaggle archive.\n","3. Adjust OUTPUT_FILE_PATH if you want a different name for the cleaned data.\n"]},{"cell_type":"markdown","id":"bac7da94","metadata":{"papermill":{"duration":0.010282,"end_time":"2025-11-26T02:42:12.669927","exception":false,"start_time":"2025-11-26T02:42:12.659645","status":"completed"},"tags":[]},"source":["<h1 style=\"font-size: 24px;\">The A2A Communication Protocol</h1>"]},{"cell_type":"markdown","id":"17c4b072","metadata":{"papermill":{"duration":0.008141,"end_time":"2025-11-26T02:42:12.685269","exception":false,"start_time":"2025-11-26T02:42:12.677128","status":"completed"},"tags":[]},"source":["<h2 style=\"font-size: 20px; font-family: 'sans-serif';\">The Agent Language (Pydantic Models):</h2>\n","\n","*These Pydantic models create a strict, JSON-based communication protocol for the AI agents, ensuring the Planner's instructions (e.g., cleaning actions) are always perfectly understood and executable by the Executor.*"]},{"cell_type":"code","execution_count":4,"id":"dc1cb123","metadata":{"execution":{"iopub.execute_input":"2025-11-26T02:42:12.704247Z","iopub.status.busy":"2025-11-26T02:42:12.70358Z","iopub.status.idle":"2025-11-26T02:42:12.769438Z","shell.execute_reply":"2025-11-26T02:42:12.768236Z"},"papermill":{"duration":0.077647,"end_time":"2025-11-26T02:42:12.771098","exception":false,"start_time":"2025-11-26T02:42:12.693451","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Agent Protocol Models Defined ---\n"]}],"source":["# ----------------------------------------------------\n","# 1. AGENT PROTOCOL DATA MODELS (Pydantic Schemas)\n","# ----------------------------------------------------\n","CleaningOperation = Literal[\"impute_missing\", \"remove_duplicates\", \"convert_datatype\", \"standardize_case\", \"drop_column\"]\n","ImputationMethod = Literal[\"mean\", \"median\", \"mode\", \"drop\"]\n","CaseMethod = Literal[\"titlecase\", \"lowercase\", \"uppercase\"]\n","DataType = Literal[\"int\", \"float\", \"datetime\", \"str\"]\n","\n","class CleaningAction(BaseModel):\n","    method: str = Field(..., description=\"The specific method for the operation (e.g., 'median', 'lowercase', 'datetime').\")\n","    operation: str = Field(..., description=\"The name of the tool/operation to execute (e.g., 'impute_missing').\")\n","    column: Optional[str] = Field(None, description=\"The specific column to target. Omit for DataFrame-wide operations.\")\n","    method: str = Field(..., description=\"The specific method for the operation (e.g., 'median', 'lowercase', 'datetime').\")\n","    rationale: str = Field(..., description=\"The LLM's reasoning for this action, used for the final report.\")\n","    execution_metrics: Dict[str, Any] = Field(default_factory=dict, description=\"Metrics generated during execution.\")\n","\n","class CleaningPlan(BaseModel):\n","    \"\"\"The complete plan generated by the Planner Agent.\"\"\"\n","    actions: List[CleaningAction] = Field(..., description=\"An ordered list of cleaning actions to be executed.\")\n","\n","class VerificationReport(BaseModel):\n","    \"\"\"The final report generated by the Verifier Agent.\"\"\"\n","    confidence_score: float = Field(..., description=\"Overall confidence score (0.0 to 1.0) in the data quality after cleaning.\")\n","    summary_of_changes: Dict[str, Any] = Field(..., description=\"Metrics like rows dropped, nulls reduced, etc.\")\n","    flags: List[str] = Field(..., description=\"List of warnings or failures identified during verification.\")\n","    recommendation: str = Field(..., description=\"Final recommendation for using the cleaned data.\")\n","\n","print(\"--- Agent Protocol Models Defined ---\")"]},{"cell_type":"markdown","id":"cb0a1f42","metadata":{"papermill":{"duration":0.007451,"end_time":"2025-11-26T02:42:12.785718","exception":false,"start_time":"2025-11-26T02:42:12.778267","status":"completed"},"tags":[]},"source":["<h1 style=\"font-size: 24px;\">Utiliy & Profiler Agent</h1>"]},{"cell_type":"markdown","id":"26d0264e","metadata":{"papermill":{"duration":0.006928,"end_time":"2025-11-26T02:42:12.799621","exception":false,"start_time":"2025-11-26T02:42:12.792693","status":"completed"},"tags":[]},"source":["<h2 style=\"font-size: 20px; font-family: 'sans-serif';\">Loader Agent: Data Ingestion and Sanitization</h2>\n","\n","*This function handles the robust downloading of the dataset from Kaggle, file path resolution, and initial data loading. Critically, it converts various common string representations of missing values (like 'N/A', '?', or empty strings) into standard np.nan for accurate downstream analysis.*"]},{"cell_type":"code","execution_count":5,"id":"a10b5af4","metadata":{"execution":{"iopub.execute_input":"2025-11-26T02:42:12.815188Z","iopub.status.busy":"2025-11-26T02:42:12.814574Z","iopub.status.idle":"2025-11-26T02:42:12.82399Z","shell.execute_reply":"2025-11-26T02:42:12.822913Z"},"papermill":{"duration":0.018902,"end_time":"2025-11-26T02:42:12.82542","exception":false,"start_time":"2025-11-26T02:42:12.806518","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Loader Agent (load_data) function defined successfully ---\n"]}],"source":["def load_data(kaggle_id: str, csv_name: str) -> Optional[pd.DataFrame]:\n","    \"\"\"\n","    [Loader Agent Logic] Downloads data from Kaggle, loads it, and robustly \n","    converts common null strings into proper np.nan for accurate profiling.\n","    \"\"\"\n","    print(f\"--- Loader Agent Activated: Downloading dataset {kaggle_id} ---\")\n","    try:\n","        # Download the dataset using kagglehub\n","        download_dir = dataset_download(kaggle_id) \n","        print(f\" > Dataset downloaded to: {download_dir}\")\n","\n","        # Use glob to find the CSV file robustly, even if nested\n","        csv_files = glob(os.path.join(download_dir, '**', csv_name), recursive=True)\n","        \n","        if csv_files:\n","            data_path = csv_files[0]\n","            print(f\" > File found at: {data_path}\")\n","        else:\n","            # Fallback: Search for *any* CSV file\n","            csv_files_wildcard = glob(os.path.join(download_dir, '**', '*.csv'), recursive=True)\n","            if csv_files_wildcard:\n","                data_path = csv_files_wildcard[0]\n","                print(f\" > WARNING: Used fallback to load first CSV found: {os.path.basename(data_path)}\")\n","            else:\n","                raise FileNotFoundError(f\"Could not find the confirmed CSV file '{csv_name}' or any other CSV file in the downloaded archive.\")\n","\n","        # Load the data, using standard pandas NA values and specifying others\n","        na_values_to_recognize = [\n","            'N/A', 'NA', 'NaN', 'null', 'None', '?', '-', ' '\n","        ]\n","        df = pd.read_csv(data_path, na_values=na_values_to_recognize)\n","        \n","        # Explicitly convert empty strings which often bypass na_values in some files\n","        df = df.replace(r'^\\s*$', np.nan, regex=True)\n","        \n","        initial_shape = df.shape\n","        initial_null_count = df.isnull().sum().sum()\n","        \n","        print(f\" > Data loaded successfully.\")\n","        print(f\" > Initial shape: {initial_shape}. Detected NaN count: {initial_null_count}\")\n","        return df\n","    except Exception as e:\n","        print(f\"!!! CRITICAL ERROR: Data ingestion failed. Error: {e}\")\n","        return None\n","print(\"--- Loader Agent (load_data) function defined successfully ---\")"]},{"cell_type":"markdown","id":"edd0038a","metadata":{"papermill":{"duration":0.006975,"end_time":"2025-11-26T02:42:12.839642","exception":false,"start_time":"2025-11-26T02:42:12.832667","status":"completed"},"tags":[]},"source":["<h2 style=\"font-size: 20px; font-family: 'sans-serif';\">Utility Function: Saving Final Output</h2>\n","\n","*This simple utility function (save_cleaned_data) is responsible for saving the final processed and cleaned DataFrame to the specified OUTPUT_FILE_PATH on disk.*"]},{"cell_type":"code","execution_count":6,"id":"c5a710a9","metadata":{"execution":{"iopub.execute_input":"2025-11-26T02:42:12.855181Z","iopub.status.busy":"2025-11-26T02:42:12.854882Z","iopub.status.idle":"2025-11-26T02:42:12.861339Z","shell.execute_reply":"2025-11-26T02:42:12.860484Z"},"papermill":{"duration":0.015961,"end_time":"2025-11-26T02:42:12.862786","exception":false,"start_time":"2025-11-26T02:42:12.846825","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Utility Function (save_cleaned_data) defined successfully ---\n"]}],"source":["def save_cleaned_data(df: pd.DataFrame, file_path: str):\n","    \"\"\"Saves the final cleaned DataFrame.\"\"\"\n","    try:\n","        OUTPUT_DIR_NAME = os.path.dirname(file_path) or \".\"\n","        os.makedirs(OUTPUT_DIR_NAME, exist_ok=True)\n","        df.to_csv(file_path, index=False)\n","        print(f\"\\n > Cleaned data successfully saved to: {file_path}\")\n","    except Exception as e:\n","        print(f\"!!! SAVER FAILED: {e}\")\n","print(\"--- Utility Function (save_cleaned_data) defined successfully ---\")"]},{"cell_type":"markdown","id":"ccd38725","metadata":{"papermill":{"duration":0.007032,"end_time":"2025-11-26T02:42:12.877193","exception":false,"start_time":"2025-11-26T02:42:12.870161","status":"completed"},"tags":[]},"source":["<h2 style=\"font-size: 20px; font-family: 'sans-serif';\">Profiler Agent: Data Observation and Summary Generation</h2>\n","\n","*The core of the observation phase, this function analyzes the loaded data, calculating critical quality metrics like total missing values, column-specific null percentages, and data types. It packages this condensed statistical summary into a JSON-like format for the Planner Agent.*"]},{"cell_type":"code","execution_count":7,"id":"15c710e2","metadata":{"execution":{"iopub.execute_input":"2025-11-26T02:42:12.893088Z","iopub.status.busy":"2025-11-26T02:42:12.892426Z","iopub.status.idle":"2025-11-26T02:42:12.90108Z","shell.execute_reply":"2025-11-26T02:42:12.9001Z"},"papermill":{"duration":0.018277,"end_time":"2025-11-26T02:42:12.902653","exception":false,"start_time":"2025-11-26T02:42:12.884376","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Profiler Agent (profile_data) function defined successfully ---\n"]}],"source":["def profile_data(df: pd.DataFrame) -> Dict[str, Any]:\n","    \"\"\"[Profiler Agent Logic] Generates a compact summary of data quality issues.\"\"\"\n","    print(\"\\n--- Profiler Agent (Observer) Activated ---\")\n","    \n","    # 1. Basic Stats\n","    total_cells = np.prod(df.shape)\n","    total_null_count = df.isnull().sum().sum()\n","\n","    # 2. Detailed Column Stats\n","    column_details = {}\n","    for col in df.columns:\n","        null_count = df[col].isnull().sum()\n","        null_percent = (null_count / len(df)) * 100\n","        unique_count = df[col].nunique()\n","        column_details[col] = {\n","            'dtype': str(df[col].dtype),\n","            'missing_values': int(null_count),\n","            'missing_percent': round(null_percent, 2),\n","            'unique_values': int(unique_count),\n","            'example_values': [str(x) for x in df[col].dropna().head(2).tolist()]\n","        }\n","\n","    # 3. Compile Summary (Context Compaction for LLM)\n","    summary = {\n","        'shape': df.shape,\n","        'total_null_percentage': round((total_null_count / total_cells) * 100, 2),\n","        'missing_values_by_column': {k: v['missing_values'] for k, v in column_details.items() if v['missing_values'] > 0},\n","        'columns': column_details\n","    }\n","    print(\" > Data profiling complete. Summary generated for Planner Agent.\")\n","    return summary\n","\n","print(\"--- Profiler Agent (profile_data) function defined successfully ---\")"]},{"cell_type":"markdown","id":"9b120e08","metadata":{"papermill":{"duration":0.00714,"end_time":"2025-11-26T02:42:12.917186","exception":false,"start_time":"2025-11-26T02:42:12.910046","status":"completed"},"tags":[]},"source":["<h1 style=\"font-size: 24px;\">Executor Tools</h1>"]},{"cell_type":"markdown","id":"248c0bc3","metadata":{"papermill":{"duration":0.007018,"end_time":"2025-11-26T02:42:12.931472","exception":false,"start_time":"2025-11-26T02:42:12.924454","status":"completed"},"tags":[]},"source":["<h2 style=\"font-size: 20px; font-family: 'sans-serif';\">Deduplication and Imputation Tools</h2>\n","\n","*Contains functions to remove duplicate rows (tool_deduplicate) and intelligently fill in missing data using mean, median, or most frequent values (tool_impute_missing).*"]},{"cell_type":"code","execution_count":8,"id":"3846e87c","metadata":{"execution":{"iopub.execute_input":"2025-11-26T02:42:12.947645Z","iopub.status.busy":"2025-11-26T02:42:12.9473Z","iopub.status.idle":"2025-11-26T02:42:12.963891Z","shell.execute_reply":"2025-11-26T02:42:12.963053Z"},"papermill":{"duration":0.02648,"end_time":"2025-11-26T02:42:12.965186","exception":false,"start_time":"2025-11-26T02:42:12.938706","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Deduplication, Imputation, Conversion, and Drop Tools defined successfully ---\n"]}],"source":["def tool_deduplicate(df: pd.DataFrame, column: Optional[str] = None, method: str = 'all') -> Tuple[pd.DataFrame, Dict[str, Any]]:\n","    \"\"\"Removes duplicate rows.\"\"\"\n","    initial_rows = len(df)\n","    df_out = df.drop_duplicates(subset=[column] if column else None, keep='first')\n","    rows_dropped = initial_rows - len(df_out)\n","    return df_out, {'rows_dropped_dedupe': rows_dropped}\n","\n","def tool_impute_missing(df: pd.DataFrame, column: str, method: str) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n","    \"\"\"Imputes missing values in a specified column.\"\"\"\n","    if column not in df.columns:\n","        return df, {'impute_status': f\"Column '{column}' not found.\"}\n","    \n","    df_temp = df.copy() \n","    missing_before = df_temp[column].isnull().sum()\n","    fill_value = None\n","    \n","    # 1. Determine the fill value based on the method\n","    if method == 'median' and pd.api.types.is_numeric_dtype(df_temp[column]):\n","        fill_value = df_temp[column].median()\n","    elif method == 'mean' and pd.api.types.is_numeric_dtype(df_temp[column]):\n","        fill_value = df_temp[column].mean()\n","    elif method == 'most_frequent':\n","        mode_val = df_temp[column].mode()\n","        # Fallback to a string placeholder if mode is empty \n","        fill_value = mode_val[0] if not mode_val.empty else 'Unknown' \n","    else:\n","        return df, {'impute_status': f\"Imputation method '{method}' is not supported for dtype or column type.\"}\n","    \n","    # 2. Apply the imputation\n","    df_temp[column] = df_temp[column].fillna(fill_value)\n","    \n","    # 3. Calculate metrics\n","    missing_after = df_temp[column].isnull().sum()\n","    imputed_count = missing_before - missing_after\n","    \n","    return df_temp, {'imputed_count': int(imputed_count)}\n","def tool_convert_datatype(df: pd.DataFrame, column: str, method: str) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n","    \"\"\"Converts a column to a specified data type, dropping rows that fail conversion.\"\"\"\n","    if column not in df.columns:\n","        return df, {'convert_status': f\"Column '{column}' not found.\"}\n","    \n","    df_temp = df.copy()\n","    initial_null_count = df_temp[column].isnull().sum()\n","    \n","    if method == 'datetime':\n","        # Errors='coerce' turns non-parsable values into NaT (Not a Time)\n","        df_temp['temp_col'] = pd.to_datetime(df_temp[column], errors='coerce') \n","        \n","    elif method == 'numeric':\n","        # Errors='coerce' turns non-parsable values into NaN (Not a Number)\n","        df_temp['temp_col'] = pd.to_numeric(df_temp[column], errors='coerce')\n","        \n","    else:\n","        return df, {'convert_status': f\"Unsupported conversion method '{method}'\"}\n","\n","    # Calculate rows dropped due to bad formatting (new NaT/NaNs)\n","    final_null_count = df_temp['temp_col'].isnull().sum()\n","    rows_dropped = final_null_count - initial_null_count\n","    \n","    # Filter out the newly generated NaT/NaNs due to bad format\n","    df_out = df_temp[df_temp['temp_col'].notnull()].copy()\n","    df_out[column] = df_out.pop('temp_col')\n","\n","    return df_out, {'rows_dropped_conversion': int(rows_dropped)}\n","def tool_drop_column(df: pd.DataFrame, column: str, method: str) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n","    \"\"\"Drops a column entirely (if column is specified) or drops sparse columns (if ALL and method='missing_gt_X').\"\"\"\n","    \n","    # 1. SPARSE COLUMN DROP (Intelligent/Data-Driven)\n","    if column == \"ALL\" and method.startswith(\"missing_gt_\"):\n","        try:\n","            # Extract threshold percentage (e.g., 'missing_gt_75' -> 0.75)\n","            threshold_percent = int(method.split('_')[-1]) / 100\n","        except ValueError:\n","            return df, {'drop_status': f\"Invalid threshold in method '{method}'\"}\n","\n","        initial_cols = df.shape[1]\n","        threshold_rows = threshold_percent * len(df)\n","        \n","        # Identify columns where NaN count exceeds the threshold\n","        cols_to_drop = df.columns[df.isnull().sum() > threshold_rows].tolist()\n","        \n","        if cols_to_drop:\n","            df_out = df.drop(columns=cols_to_drop)\n","            cols_dropped_count = initial_cols - df_out.shape[1]\n","            return df_out, {'columns_dropped_sparse': cols_dropped_count, 'dropped_names': cols_to_drop}\n","        else:\n","            return df, {'columns_dropped_sparse': 0}\n","\n","    # 2. EXPLICIT COLUMN DROP\n","    elif column in df.columns:\n","        # Standard explicit column drop\n","        df_out = df.drop(columns=[column])\n","        return df_out, {'columns_dropped_explicit': 1, 'dropped_names': [column]}\n","        \n","    # 3. FAILURE/ERROR\n","    return df, {'drop_status': f\"Column '{column}' not found or unsupported drop method.\"}\n","print(\"--- Deduplication, Imputation, Conversion, and Drop Tools defined successfully ---\")"]},{"cell_type":"markdown","id":"3aeeb773","metadata":{"papermill":{"duration":0.007055,"end_time":"2025-11-26T02:42:12.979681","exception":false,"start_time":"2025-11-26T02:42:12.972626","status":"completed"},"tags":[]},"source":["<h2 style=\"font-size: 20px; font-family: 'sans-serif';\">Conversion and Standardization Tools</h2>\n","\n","*Includes functions to handle structural issues, converting columns to correct data types like datetime or numeric, and standardizing text casing (title, lower, upper) for consistency.*"]},{"cell_type":"code","execution_count":9,"id":"ad8ef438","metadata":{"execution":{"iopub.execute_input":"2025-11-26T02:42:12.99544Z","iopub.status.busy":"2025-11-26T02:42:12.995066Z","iopub.status.idle":"2025-11-26T02:42:13.00416Z","shell.execute_reply":"2025-11-26T02:42:13.003251Z"},"papermill":{"duration":0.018833,"end_time":"2025-11-26T02:42:13.00564","exception":false,"start_time":"2025-11-26T02:42:12.986807","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Conversion and Standardization Tools defined successfully ---\n"]}],"source":["def tool_standardize_case(df: pd.DataFrame, column: str, method: str) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n","    \"\"\"Applies a specified case transformation (lower, upper, title) to a text column, preserving NaN values.\"\"\"\n","    if column not in df.columns:\n","        return df, {'case_status': f\"Column '{column}' not found.\"}\n","    \n","    # 1. Check if the column is a string type (or object)\n","    if not (pd.api.types.is_object_dtype(df[column]) or pd.api.types.is_string_dtype(df[column])):\n","        return df, {'case_status': f\"Skipped: Column '{column}' is not a string type.\"}\n","\n","    df_temp = df.copy()\n","    initial_unique_count = df_temp[column].nunique(dropna=False) # Count NaNs as unique\n","    \n","    # 2. Convert to string and apply case transformation\n","    series = df_temp[column].astype(str)\n","    \n","    if method == 'lowercase':\n","        df_temp[column] = series.str.lower()\n","    elif method == 'uppercase':\n","        df_temp[column] = series.str.upper()\n","    elif method == 'titlecase':\n","        df_temp[column] = series.str.title()\n","    else:\n","        return df, {'case_status': f\"Unsupported case method: {method}\"}\n","\n","    # 3. CRITICAL STEP from Impl. 2: Convert the literal 'nan' string back to proper np.nan\n","    df_temp[column] = df_temp[column].replace({'nan': np.nan})\n","    \n","    # 4. Metric calculation from Impl. 1\n","    final_unique_count = df_temp[column].nunique(dropna=False)\n","    reduction = initial_unique_count - final_unique_count\n","    \n","    return df_temp, {\n","        'unique_reduction_case': reduction, \n","        'method_applied': method,\n","        'case_status': f\"Case standardized to {method}\"\n","    }\n","print(\"--- Conversion and Standardization Tools defined successfully ---\")"]},{"cell_type":"markdown","id":"0857af83","metadata":{"papermill":{"duration":0.00714,"end_time":"2025-11-26T02:42:13.020248","exception":false,"start_time":"2025-11-26T02:42:13.013108","status":"completed"},"tags":[]},"source":["<h2 style=\"font-size: 20px; font-family: 'sans-serif';\">Tool Mapping</h2>\n","\n","*This dictionary maps the standardized operation names (defined in the CleaningAction Pydantic model) to the corresponding tool functions for dynamic execution.*"]},{"cell_type":"code","execution_count":10,"id":"7f67cb60","metadata":{"execution":{"iopub.execute_input":"2025-11-26T02:42:13.03664Z","iopub.status.busy":"2025-11-26T02:42:13.036008Z","iopub.status.idle":"2025-11-26T02:42:13.040779Z","shell.execute_reply":"2025-11-26T02:42:13.040014Z"},"papermill":{"duration":0.014375,"end_time":"2025-11-26T02:42:13.042244","exception":false,"start_time":"2025-11-26T02:42:13.027869","status":"completed"},"tags":[]},"outputs":[],"source":["# Map of tool names to functions\n","TOOL_MAP: Dict[str, Callable] = {\n","    'deduplicate': tool_deduplicate,\n","    'impute_missing': tool_impute_missing,\n","    'convert_datatype': tool_convert_datatype,\n","    'standardize_case': tool_standardize_case,\n","    'drop_column': tool_drop_column,\n","}"]},{"cell_type":"markdown","id":"6ae6201b","metadata":{"papermill":{"duration":0.007128,"end_time":"2025-11-26T02:42:13.056869","exception":false,"start_time":"2025-11-26T02:42:13.049741","status":"completed"},"tags":[]},"source":["<h1 style=\"font-size: 24px;\">Executor Agent</h1>"]},{"cell_type":"markdown","id":"9257c85e","metadata":{"papermill":{"duration":0.007036,"end_time":"2025-11-26T02:42:13.071195","exception":false,"start_time":"2025-11-26T02:42:13.064159","status":"completed"},"tags":[]},"source":["*The executor_agent_execute function sequentially processes the LLM's cleaning plan, dynamically calling the appropriate tool functions from the TOOL_MAP to perform all data transformations.*"]},{"cell_type":"code","execution_count":11,"id":"75e347db","metadata":{"execution":{"iopub.execute_input":"2025-11-26T02:42:13.087675Z","iopub.status.busy":"2025-11-26T02:42:13.086842Z","iopub.status.idle":"2025-11-26T02:42:13.095159Z","shell.execute_reply":"2025-11-26T02:42:13.094212Z"},"papermill":{"duration":0.018437,"end_time":"2025-11-26T02:42:13.096944","exception":false,"start_time":"2025-11-26T02:42:13.078507","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Executor Agent Logic Defined ---\n"]}],"source":["def executor_agent_execute(df_raw: pd.DataFrame, cleaning_plan: CleaningPlan) -> pd.DataFrame:\n","    \"\"\"Executes the cleaning plan, returning the cleaned DataFrame.\"\"\"\n","    print(\"\\n--- Executor Agent (The Doer) Activated ---\")\n","    df_cleaned = df_raw.copy() \n","    \n","    for i, action in enumerate(cleaning_plan.actions):\n","        tool_name = action.operation\n","        \n","        if tool_name in TOOL_MAP:\n","            tool_func = TOOL_MAP[tool_name]\n","            \n","            kwargs = {}\n","            # Pass column if present (it's Optional in Pydantic)\n","            if action.column is not None:\n","                kwargs['column'] = action.column\n","            \n","            # Pass method (it's required in Pydantic, but this ensures it's available)\n","            kwargs['method'] = action.method\n","            try:\n","                df_cleaned, metrics = tool_func(df_cleaned, **kwargs)\n","                print(f\" > Executed {i+1}: {tool_name} on {action.column if action.column else 'DataFrame'} ({action.method}).\")\n","                action.execution_metrics = metrics \n","            except Exception as e:\n","                print(f\"!!! EXECUTION FAILED for {tool_name} on {action.column}: {e}\")\n","                action.execution_metrics = {'error': str(e)}\n","        else:\n","            print(f\"!!! Executor: Unknown operation '{tool_name}' in plan. Skipping action {i+1}.\")\n","            action.execution_metrics = {'error': f\"Unknown tool: {tool_name}\"}\n","\n","    return df_cleaned\n","\n","print(\"--- Executor Agent Logic Defined ---\")"]},{"cell_type":"markdown","id":"ecd2b1fc","metadata":{"papermill":{"duration":0.007786,"end_time":"2025-11-26T02:42:13.1124","exception":false,"start_time":"2025-11-26T02:42:13.104614","status":"completed"},"tags":[]},"source":["\n","<h1 style=\"font-size: 24px;\">Verifier Agent</h1>"]},{"cell_type":"markdown","id":"75a1dd05","metadata":{"papermill":{"duration":0.007305,"end_time":"2025-11-26T02:42:13.127393","exception":false,"start_time":"2025-11-26T02:42:13.120088","status":"completed"},"tags":[]},"source":["*The Verifier Agent is the quality assurance step, comparing the raw and cleaned data to generate a final VerificationReport. It calculates the confidence score based on null reduction, row loss, and execution success, providing a recommendation for the data's usability.*"]},{"cell_type":"code","execution_count":12,"id":"734b1881","metadata":{"execution":{"iopub.execute_input":"2025-11-26T02:42:13.143473Z","iopub.status.busy":"2025-11-26T02:42:13.143154Z","iopub.status.idle":"2025-11-26T02:42:13.15437Z","shell.execute_reply":"2025-11-26T02:42:13.153384Z"},"papermill":{"duration":0.021076,"end_time":"2025-11-26T02:42:13.155801","exception":false,"start_time":"2025-11-26T02:42:13.134725","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Verifier Agent Logic Defined ---\n"]}],"source":["def verifier_agent_execute(df_raw: pd.DataFrame, df_cleaned: pd.DataFrame, cleaning_plan: CleaningPlan) -> VerificationReport:\n","    \"\"\"[Verifier Agent] Compares raw and cleaned data and generates the final report.\"\"\"\n","    print(\"\\n--- Verifier Agent (Quality Assurance) Activated ---\")\n","\n","    raw_rows = len(df_raw)\n","    cleaned_rows = len(df_cleaned)\n","    # Ensure null counts are cast to standard Python integers/floats immediately\n","    raw_nulls = df_raw.isnull().sum().sum()\n","    cleaned_nulls = df_cleaned.isnull().sum().sum()\n","    \n","    # Calculate key metrics\n","    rows_dropped_total = raw_rows - cleaned_rows\n","    nulls_removed = raw_nulls - cleaned_nulls\n","\n","    summary = {\n","        'initial_rows': raw_rows,\n","        'final_rows': cleaned_rows,\n","        'rows_dropped_total': rows_dropped_total,\n","        'initial_null_count': int(raw_nulls),\n","        'final_null_count': int(cleaned_nulls),\n","        'nulls_removed': int(nulls_removed),\n","        'actions_performed': [f\"{a.operation}({a.column or 'df'}, {a.method})\" for a in cleaning_plan.actions]\n","    }\n","    \n","    flags: List[str] = []\n","    \n","    # 1. Check for Excessive Row Deletion\n","    row_loss_percent = (rows_dropped_total / raw_rows) * 100 if raw_rows > 0 else 0\n","    if row_loss_percent > 15:\n","        flags.append(f\"WARNING: High row deletion ({row_loss_percent:.1f}%) detected.\")\n","        \n","    # 2. Check for Execution Errors\n","    for action in cleaning_plan.actions:\n","        metrics = action.execution_metrics\n","        if 'error' in metrics:\n","            flags.append(f\"ERROR: Execution failed for {action.operation} on {action.column}: {metrics['error']}\")\n","            \n","    # 3. Determine Confidence Score (Max 1.0)\n","    score = 0.0\n","    \n","    # a) Null Reduction (Max 50 points, scaled to 0.5)\n","    if raw_nulls > 0:\n","        # Ratio is capped at 1.0 (if all nulls are removed)\n","        null_reduction_ratio = min(1.0, nulls_removed / raw_nulls)\n","    else:\n","        # If there were no raw nulls, assume 100% success (ratio = 1.0)\n","        null_reduction_ratio = 1.0\n","        \n","    score += 0.5 * null_reduction_ratio # Max 0.5\n","        \n","    # b) Row Loss Penalty/Reward (Max 30 points, scaled to 0.3)\n","    if row_loss_percent < 5:\n","        score += 0.30 \n","    elif row_loss_percent < 15:\n","        score += 0.15 # Mid-level penalty\n","    # If row_loss_percent >= 15, score gets 0 points from this category\n","\n","    # c) Execution Success (Max 20 points, scaled to 0.2)\n","    if not any('ERROR' in flag for flag in flags):\n","        score += 0.20 \n","\n","    # Final score is capped at 1.0\n","    confidence_score = min(1.0, score)\n","\n","    # 4. Final Recommendation\n","    if confidence_score > 0.85:\n","        recommendation = \"Data quality is excellent. Ready for machine learning model training.\"\n","    elif confidence_score > 0.60:\n","        recommendation = \"Data quality is acceptable. Review the flags for potential minor issues before use.\"\n","    else:\n","        recommendation = \"Data quality is poor or plan execution failed. Manual intervention is required.\"\n","\n","    print(\" > Verification complete. Confidence score calculated.\")\n","    \n","    return VerificationReport(\n","        confidence_score=confidence_score,\n","        summary_of_changes=summary,\n","        flags=flags,\n","        recommendation=recommendation\n","    )\n","\n","print(\"--- Verifier Agent Logic Defined ---\")"]},{"cell_type":"markdown","id":"8cca22af","metadata":{"papermill":{"duration":0.007415,"end_time":"2025-11-26T02:42:13.170873","exception":false,"start_time":"2025-11-26T02:42:13.163458","status":"completed"},"tags":[]},"source":["<h1 style=\"font-size: 24px;\">Planner Agent</h1>"]},{"cell_type":"markdown","id":"df7db286","metadata":{"papermill":{"duration":0.007315,"end_time":"2025-11-26T02:42:13.185695","exception":false,"start_time":"2025-11-26T02:42:13.17838","status":"completed"},"tags":[]},"source":["\n","*Planner Agent attempts a live call to the Gemini LLM for zero-shot cleaning plan generation using the data profile and strict JSON schema. If the live call fails, it executes a robust, hardcoded fallback plan to guarantee a usable cleaning sequence.*\n"]},{"cell_type":"code","execution_count":13,"id":"4026aa4f","metadata":{"execution":{"iopub.execute_input":"2025-11-26T02:42:13.201923Z","iopub.status.busy":"2025-11-26T02:42:13.201612Z","iopub.status.idle":"2025-11-26T02:42:13.210814Z","shell.execute_reply":"2025-11-26T02:42:13.209937Z"},"papermill":{"duration":0.019177,"end_time":"2025-11-26T02:42:13.21223","exception":false,"start_time":"2025-11-26T02:42:13.193053","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Planner Agent Logic Defined ---\n"]}],"source":["# ----------------------------------------------------\n","# 5. PLANNER AGENT LOGIC\n","# ----------------------------------------------------\n","\n","def planner_agent_execute_adk(data_summary: Dict[str, Any]) -> CleaningPlan:\n","    \"\"\"[Planner Agent - Static Version] Returns a structured CleaningPlan using a hardcoded dictionary.\"\"\"\n","    print(\"\\n--- Planner Agent Activated ---\")\n","    \n","    # --- HARDCODED FALLBACK PLAN (The \"function json\") ---\n","    fallback_plan_data = {\n","        \"actions\": [\n","            {\"operation\": \"deduplicate\", \"column\": \"title\", \"method\": \"first\", \"rationale\": \"Remove exact duplicate movies based on the primary identifier (Title).\"},\n","            {\"operation\": \"convert_datatype\", \"column\": \"release_date\", \"method\": \"datetime\", \"rationale\": \"Ensure release date is a proper datetime format, dropping bad rows.\"},\n","            \n","            # Targeted Imputation for Common Nulls\n","            {\"operation\": \"impute_missing\", \"column\": \"vote_average\", \"method\": \"median\", \"rationale\": \"Impute missing numerical ratings with the dataset median.\"},\n","            {\"operation\": \"impute_missing\", \"column\": \"runtime\", \"method\": \"mean\", \"rationale\": \"Impute missing numerical runtimes with the mean.\"},\n","            {\"operation\": \"impute_missing\", \"column\": \"overview\", \"method\": \"most_frequent\", \"rationale\": \"Impute missing text descriptions using the most common description/overview.\"},\n","            {\"operation\": \"impute_missing\", \"column\": \"budget\", \"method\": \"median\", \"rationale\": \"Impute missing Budget (assuming numeric) with the median.\"},\n","            \n","            # Standardize text fields for quality\n","            {\"operation\": \"standardize_case\", \"column\": \"genres\", \"method\": \"titlecase\", \"rationale\": \"Ensure text fields like genres are consistently formatted (Title Case).\"},\n","        ]\n","    }\n","    # ----------------------------------------------------\n","    \n","    llm_response_data = fallback_plan_data\n","    print(\" > Static fallback plan is active and loaded.\")\n","            \n","    # --- Final validation and return ---\n","    if llm_response_data is None:\n","        # This case should no longer be possible with the static plan\n","        print(\"!!! Planner FAILED: No valid plan data available.\")\n","        return CleaningPlan(actions=[])\n","\n","    try:\n","        # Validate the static data against the Pydantic schema\n","        cleaning_plan = CleaningPlan.model_validate(llm_response_data)\n","        print(\" > Cleaning Plan generated and validated successfully.\")\n","        print(f\" > Generated {len(cleaning_plan.actions)} cleaning actions.\")\n","        return cleaning_plan\n","    except Exception as e:\n","        print(f\"!!! PLANNER FAILED: Plan data did not conform to the CleaningPlan schema: {e}. Returning empty plan.\")\n","        return CleaningPlan(actions=[])\n","print(\"--- Planner Agent Logic Defined ---\")"]},{"cell_type":"markdown","id":"77469fde","metadata":{"papermill":{"duration":0.007298,"end_time":"2025-11-26T02:42:13.227292","exception":false,"start_time":"2025-11-26T02:42:13.219994","status":"completed"},"tags":[]},"source":["**Imp Note:**\n","\n","\n","***Fallback Customization Instruction:** This warning ensures the hardcoded fallback plan's column names and methods are manually updated when changing datasets, guaranteeing the fallback remains logically relevant.*\n"]},{"cell_type":"markdown","id":"6fc00b13","metadata":{"papermill":{"duration":0.007205,"end_time":"2025-11-26T02:42:13.241884","exception":false,"start_time":"2025-11-26T02:42:13.234679","status":"completed"},"tags":[]},"source":["<h1 style=\"font-size: 24px;\">Main Pipeline Execution</h1>"]},{"cell_type":"markdown","id":"ed203bac","metadata":{"papermill":{"duration":0.007214,"end_time":"2025-11-26T02:42:13.2565","exception":false,"start_time":"2025-11-26T02:42:13.249286","status":"completed"},"tags":[]},"source":["\n","*This function (run_pipeline) orchestrates the entire multi-agent process, executing the Loader, Profiler, Planner, Executor, and Verifier in sequence, concluding by displaying a detailed final verification report and saving the cleaned data.*\n"]},{"cell_type":"code","execution_count":14,"id":"1e4abb09","metadata":{"execution":{"iopub.execute_input":"2025-11-26T02:42:13.272961Z","iopub.status.busy":"2025-11-26T02:42:13.27263Z","iopub.status.idle":"2025-11-26T02:42:13.875087Z","shell.execute_reply":"2025-11-26T02:42:13.873676Z"},"papermill":{"duration":0.612796,"end_time":"2025-11-26T02:42:13.876648","exception":false,"start_time":"2025-11-26T02:42:13.263852","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","=======================================================\n","  ZERO-SHOT DATA TRANSFORMATION: PIPELINE START (ADK)\n","=======================================================\n","--- Loader Agent Activated: Downloading dataset praveensoni06/1500-latest-movies-datasets-2025 ---\n"," > Dataset downloaded to: /kaggle/input/1500-latest-movies-datasets-2025\n"," > File found at: /kaggle/input/1500-latest-movies-datasets-2025/Latest 2025 movies Datasets.csv\n"," > Data loaded successfully.\n"," > Initial shape: (10000, 8). Detected NaN count: 291\n","\n","--- Profiler Agent (Observer) Activated ---\n"," > Data profiling complete. Summary generated for Planner Agent.\n","\n","--- Planner Agent Activated ---\n"," > Static fallback plan is active and loaded.\n"," > Cleaning Plan generated and validated successfully.\n"," > Generated 7 cleaning actions.\n","\n","--- Executor Agent (The Doer) Activated ---\n"," > Executed 1: deduplicate on title (first).\n"," > Executed 2: convert_datatype on release_date (datetime).\n"," > Executed 3: impute_missing on vote_average (median).\n"," > Executed 4: impute_missing on runtime (mean).\n"," > Executed 5: impute_missing on overview (most_frequent).\n"," > Executed 6: impute_missing on budget (median).\n"," > Executed 7: standardize_case on genres (titlecase).\n","\n","--- Verifier Agent (Quality Assurance) Activated ---\n"," > Verification complete. Confidence score calculated.\n","\n","=======================================================\n","          A.I. DATA CLEANING VERIFICATION REPORT         \n","=======================================================\n","Confidence Score: 0.70\n","\n","Summary of Changes:\n","{\n","  \"initial_rows\": 10000,\n","  \"final_rows\": 7548,\n","  \"rows_dropped_total\": 2452,\n","  \"initial_null_count\": 291,\n","  \"final_null_count\": 0,\n","  \"nulls_removed\": 291,\n","  \"actions_performed\": [\n","    \"deduplicate(title, first)\",\n","    \"convert_datatype(release_date, datetime)\",\n","    \"impute_missing(vote_average, median)\",\n","    \"impute_missing(runtime, mean)\",\n","    \"impute_missing(overview, most_frequent)\",\n","    \"impute_missing(budget, median)\",\n","    \"standardize_case(genres, titlecase)\"\n","  ]\n","}\n","\n","Detailed Action Metrics:\n"," - deduplicate (title): rows_dropped_dedupe: 2415\n"," - convert_datatype (release_date): rows_dropped_conversion: 0\n"," - impute_missing (vote_average): imputed_count: 0\n"," - impute_missing (runtime): impute_status: Column 'runtime' not found.\n"," - impute_missing (overview): imputed_count: 177\n"," - impute_missing (budget): impute_status: Column 'budget' not found.\n"," - standardize_case (genres): case_status: Column 'genres' not found.\n","\n","Flags/Warnings:\n"," - WARNING: High row deletion (24.5%) detected.\n","\n","Final Recommendation:\n"," - Data quality is acceptable. Review the flags for potential minor issues before use.\n","=======================================================\n","\n"," > Cleaned data successfully saved to: cleaned_movies_data.csv\n"]}],"source":["# ----------------------------------------------------\n","# 6. MAIN PIPELINE EXECUTION\n","# ----------------------------------------------------\n","\n","def run_pipeline():\n","    \"\"\"Executes the entire 5-step Multi-Agent System sequentially.\"\"\"\n","    print(f\"\\n=======================================================\")\n","    print(f\"  ZERO-SHOT DATA TRANSFORMATION: PIPELINE START (ADK)\")\n","    print(f\"=======================================================\")\n","\n","    # Step 1: Load Data (Loader Agent)\n","    df_raw = load_data(KAGGLE_DATASET_ID, CSV_FILE_NAME)\n","\n","    if df_raw is None:\n","        print(\"\\n*** ABORTING PIPELINE due to data loading failure. ***\")\n","        return\n","\n","    # Step 2: Profile Data (Profiler Agent)\n","    summary_raw = profile_data(df_raw)\n","    \n","    # Step 3: Plan Generation (Planner Agent)\n","    cleaning_plan = planner_agent_execute_adk(summary_raw)\n","\n","    if cleaning_plan.actions:\n","        # Step 4: Execution (Executor Agent)\n","        df_cleaned = executor_agent_execute(df_raw, cleaning_plan)\n","\n","        # Step 5: Verification and Reporting (Verifier Agent)\n","        verification_report = verifier_agent_execute(df_raw, df_cleaned, cleaning_plan)\n","        \n","        # Display Final Report\n","        print(\"\\n=======================================================\")\n","        print(\"          A.I. DATA CLEANING VERIFICATION REPORT         \")\n","        print(\"=======================================================\")\n","        print(f\"Confidence Score: {verification_report.confidence_score:.2f}\")\n","        \n","        print(\"\\nSummary of Changes:\")\n","        print(json.dumps(verification_report.summary_of_changes, indent=2))\n","        \n","        print(\"\\nDetailed Action Metrics:\")\n","        for action in cleaning_plan.actions:\n","            metrics = action.execution_metrics\n","            metric_str = \", \".join([f\"{k}: {v}\" for k, v in metrics.items()])\n","            print(f\" - {action.operation} ({action.column or 'df'}): {metric_str}\")\n","\n","        print(\"\\nFlags/Warnings:\")\n","        if verification_report.flags:\n","            for flag in verification_report.flags:\n","                print(f\" - {flag}\")\n","        else:\n","            print(\" - None\")\n","            \n","        print(\"\\nFinal Recommendation:\")\n","        print(f\" - {verification_report.recommendation}\")\n","        print(\"=======================================================\")\n","\n","        # Final Step: Save Results (Utility)\n","        save_cleaned_data(df_cleaned, OUTPUT_FILE_PATH)\n","    else:\n","        print(\"\\n*** ABORTING PIPELINE: Planner failed to generate a valid plan. ***\")\n","\n","if __name__ == \"__main__\":\n","    run_pipeline()"]},{"cell_type":"code","execution_count":null,"id":"73be1b76","metadata":{"papermill":{"duration":0.007634,"end_time":"2025-11-26T02:42:13.892667","exception":false,"start_time":"2025-11-26T02:42:13.885033","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":14448097,"datasetId":8717198,"isSourceIdPinned":false,"sourceId":13703496,"sourceType":"datasetVersion"}],"dockerImageVersionId":31192,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":11.5327,"end_time":"2025-11-26T02:42:14.420023","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-26T02:42:02.887323","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}